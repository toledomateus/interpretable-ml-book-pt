# Ceteris Paribus Plots {#ceteris-paribus}

{{< include _setup.qmd >}}

Ceteris paribus (CP) plots [@kuzba2019pyceterisparibus] visualize how changes in a single feature change the prediction of a data point.

<!-- introduction -->
Ceteris paribus plots are one of the simplest analysis one can do, despite the complex-sounding Latin name, which stands for "other things equal" and means changing one feature but keeping the others untouched.[^hate-latin]
It's so simple since it only looks at one feature at a time, systematically changes its values, and plots how the prediction changes across the range of the feature.
But ceteris paribus is the perfect model-agnostic method to start the book with, since it teaches the basic principles of model-agnostic interpretation.
Also, don't be deceived by the simplicity:
By creatively combining multiple ceteris paribus curves[^curve-vs-plot], you can compare models, features, and study multi-class classification models.
CP curves are building blocks for [Individual Conditional Expectation curves](#ice) and [Partial Dependence Plots](#pdp), as visualized in @fig-cp-ice-pdp.

- ICE plots are CP plots containing all CP curves for an entire dataset.
- A partial dependence plot (PDP) is the average of all CP curves of one dataset.

![Ceteris Paribus, ICE, and PDP.](./images/cp-ice-pdp.jpg){#fig-cp-ice-pdp}

## Algorithm

<!-- Ceteris Paribus algorithm -->
Let's get started with the ceteris paribus algorithm.
This is also a little showcase of how things may seem more complex than they are when you use math.
The following algorithm is for numerical features:

Input: Data point $\mathbf{x}^{(i)}$ to explain and feature $j$

1. Create an equidistant value grid: $z_1, \ldots, z_K$, where typically $z_1 = \min(\mathbf{x}_j)$ and $z_K = \max(\mathbf{x}_j)$.
1. For each grid value $z_k \in \{z_1, \ldots, z_K\}$:
   1. Create new data point $\mathbf{x}^{(i)}_{x_j := z_k}$
   1. Get prediction $\hat{f}(\mathbf{x}^{(i)}_{x_j := z_k})$
1. Visualize the CP curves:
     - Plot line for data points $\left\{z_l, \hat{f}(\mathbf{x}^{(i)}_{x_j := z_k})\right\}_{k=1}^K$
     - Plot dot for original data point $\left(\mathbf{x}_k^{(i)}, \hat{f}(\mathbf{x}^{(i)})\right)$


The more grid values, the more fine-grained the CP curve, but the more calls you have to make to the predict function of the model.
And for a categorical feature:

1. Create list of unique categories $z_1, \ldots, z_K$
1. For each category $z_k \in \{z_1, \ldots, z_K\}$:
   1. Create new data point $\mathbf{x}^{(i)}_{x_j := z_k}$
   1. Get prediction $\hat{f}(\mathbf{x}^{(i)}_{x_j := z_k})$
1. Create bar plot or dot plot with categories on x-axis and predictions on y-axis.

But that sounded more complex than necessary.
Let's make CP plots more concrete with a few examples.

## Examples

```{r}
#| label: pick penguin
data_point <- penguins_test[1,]
data_point_y = penguins_test[1,]$sex
```

For our first example, we look at the random forest predicting the probability of a penguin being female.
We look at the first penguin in the test dataset, a  `r as.character(data_point$species)` penguin with a bill depth of `r data_point["bill_depth_mm"]` millimeters (ground truth `r as.character(data_point_y)`).
@fig-ceteris-penguins shows that decreasing the bill depth of this penguin first slightly increases the predicted P(female), but then greatly decreases P(female).

```{r}
#| label: fig-ceteris-penguins
#| fig-cap: Ceteris paribus plot for bill depth and one penguin. The line shows the predicted value for a particular penguin when changing the bill depth. The penguin's actual bill depth is marked with a dot.
#| out-width: 85%
pred2 <- function(m, x) predict(m, x, type = "prob")[,2]
explainer <- DALEX::explain(pengu_rf, data = penguins_test,
                            y = penguins_test$sex == "female",
                            predict_function = pred2,
                            label = "female", verbose=FALSE)



cp_rf <- ceteris_paribus(explainer, data_point, y = data_point_y)
plot(cp_rf,  selected_variables="bill_depth_mm") +
  xlab("Bill depth in mm") +
  scale_y_continuous("P(female)", limits = c(0,1)) +
  ggtitle("")
```

Since it's a binary classification task, visualizing $\mathbb{P}(Y = \text{male})$ is redundant -- it would just be the inverted $\mathbb{P}(Y = \text{female})$ plot.
But if we had more than two classes, we could plot the ceteris paribus curves for all the classes in one plot.

```{r}
explainer <- DALEX::explain(bike_svm, data = bike_test[colnames(bike) != "cnt"], y = bike_test$cnt, verbose=FALSE)
observation = bike_test[1,]
```

Again, we have to keep in mind that changing a feature can break the dependence with other features.
Looking at correlation and other dependence measures can be helpful.
Bill depth is correlated with body mass and flipper length.
So when looking at @fig-ceteris-penguins, we should keep in mind not to over-interpret strong reductions in bill depth in this ceteris paribus plot.

::: {.callout-warning}

# Look beyond pairwise dependencies

@fig-pairwise-not-enough shows an example where we artificially change the bill depth feature of the lightest Gentoo penguin.
The data point is realistic when we only look at the combination of body mass and bill depth.
It's also realistic when we only look at bill depth and species.
However, it's unrealistic when considering the new bill depth, body mass, and species together.

```{r}
#| label: fig-pairwise-not-enough
#| fig-cap: Scatter plot of bill depth and body mass. Manipulating a penguins bill depth can create an unrealistic data point. 
#| out-width: 85%

dtpt <- penguins %>%
        filter(species == "Gentoo") %>%
        filter(body_mass_g == min(body_mass_g))

ggplot(data = penguins, aes(x = bill_depth_mm, y = body_mass_g, color = species)) +
  annotate("segment", x = dtpt$bill_depth_mm, xend = dtpt$bill_depth_mm + 3.3,
                      y = dtpt$body_mass_g, yend=dtpt$body_mass_g, size=1,
           arrow = arrow(length = unit(0.5, "cm"))) +
  geom_point(data = dtpt, color="red", fill="red", shape=21, size = 4) + 
  geom_point() +
  xlab("Bill depth in mm") +
  ylab("Body mass in grams") +
  scale_color_viridis_d()

```

:::


Next, we study the SVM predicting the number of rented bikes based on weather and seasonal information.
We pick a `r tolower(observation$season)` day with `r tolower(observation$weather)` weather and see how changing the features would change the prediction.
But this time we visualize it for all features, see @fig-whatif-bike.
Changes in the number of rented bikes 2 days before would change the prediction the most.
Also, a higher temperature would have been better for more bike rentals.
Were the prediction for a non-workday, the SVM would predict fewer bike rentals.

```{r}
#| label: fig-whatif-bike
#| fig-cap: How changing individual features changes the predicted number of bike rentals.
#| fig-width: 8
library(e1071)
prof = predict_profile(explainer, new_observation=observation)
p1 = plot(prof, variable_type="categorical", categorical_type = "bars")
p2 = plot(prof)
p1 + p2
```


::: {.callout-tip}

## Minimal version with sparklines

CP plots can be packaged into sparklines, a minimalistic line-plot popularized by @tufte1983visual.

```{r}
#| out-width: 100%
#| fig-align: center
#| fig-asp: 0.13
numeric_features <- names(bike_test)[sapply(bike, is.numeric)]
numeric_features <- setdiff(numeric_features, "cnt")

wi_dat = data.frame(prof)

plot_data = lapply(numeric_features, function(feat){
  dat <- wi_dat[wi_dat["X_vname_"] == feat, ]
  data.frame(fvalue = dat[[feat]], yhat = dat[["X_yhat_"]], fname = feat)
})

plot_data = do.call(rbind, plot_data)
point_data = melt(observation[numeric_features], variable.name="fname", value.name="fvalue")
point_data$yhat = predict(bike_svm, observation)

ggplot(mapping=aes(x=fvalue, y=yhat)) +
  geom_line(data=plot_data) +
  geom_point(data = point_data) + 
  theme_void() +
  facet_grid(. ~ fname, scales="free_x") 
```
:::

In general, the CP plots show how feature changes affect the prediction, from small changes to large changes in all directions.
By comparing all the features side-by-side with a shared y-axis, we can see which feature has more influence on this data point's prediction.
However, correlation between features is a concern, especially when interpreting the CP curve far away from the original value (marked with a dot).
For example, increasing the temperature to 30 degrees Celsius, but keeping the season the same (`r tolower(observation$season)`) would be quite unrealistic.

Ceteris paribus plots are super flexible.
We can compute CP curves for different models to better understand how they treat features differently.
In @fig-cp-bike-models, we compare the ceteris paribus plots for different models.

```{r}
#| label: fig-cp-bike-models 
#| fig-cap: "Ceteris paribus curves for the bike prediction task for different models: linear model, random forest, SVM, and decision tree."
#| out-width: 85%
explainer_rf <- DALEX::explain(bike_rf, data = bike_test[colnames(bike) != "cnt"], y = bike_test$cnt, verbose=FALSE)
explainer_cart <- DALEX::explain(bike_tree, data = bike_test[colnames(bike) != "cnt"], y = bike_test$cnt, verbose=FALSE, label="tree")
explainer_lm <- DALEX::explain(bike_lm, data = bike_test[colnames(bike) != "cnt"], y = bike_test$cnt, verbose=FALSE)
explainer_svm  <- DALEX::explain(bike_svm, data = bike_test[colnames(bike) != "cnt"], y = bike_test$cnt, verbose=FALSE)
cp_rf = ceteris_paribus(explainer_rf, observation, y = observation$cnt)
cp_cart = ceteris_paribus(explainer_cart, observation, y = observation$cnt)
cp_lm = ceteris_paribus(explainer_lm, observation, y = observation$cnt)
cp_svm = ceteris_paribus(explainer_svm, observation, y = observation$cnt)

p = plot(cp_rf, cp_cart, cp_lm, cp_svm, color = "_label_", variables ="temp", alpha=1) +
  xlab("Temperature in C") +
  ylab("Predicted bike rentals") +
  scale_color_discrete("Model") +
  scale_linetype_discrete("Model") +
  ggtitle("") +
  aes(linetype=`_label_`)
p
```

Here we can see how very different the models behave:
The linear model does what linear models do and models the relation linearly between temperature and predicted bike rentals.
The tree shows one jump.
The random forest and the SVM model show a smoother increase, which flattens at high temperature, and, in the case of the SVM, slightly decreases for very high temperatures.

::: {.callout-tip}

# Be creative with comparisons

Ceteris paribus plots are simple, yet surprisingly insightful when you combine multiple CP curves:

- Compare features.
- Compare models from different machine learning algorithms or with different hyperparameter settings.
- Compare class probabilities.
- Compare different data points (see also [ICE curves](#ice)).
- Subset the data (e.g., by a binary feature) and compare CP curves.

:::



<!--
There's also a close relation to [counterfactual explanations](#counterfactuals).
Counterfactual explanations define a desired output (prediction) and then search for feature changes that achieve that desired outcome.
Ceteris paribus are also "What-If" tools, but do the much simpler inverse and do changes in the features and observe what the outcome is.
You could actually use Ceteris Paribus Plots to search for counterfactual explanations that are just based on single feature changes.
Ceteris paribus are also related to a method from sensitivity analysis called one-at-a-time (OAT).

Ceteris paribus analysis is also a fix to some problems of attribution-based techniques like [shap](#shap), and to some degree also [LIME](#lime).
These attribution techniques are also for explaining individual predictions.
Each feature gets a value, and the values should sum up to the prediction (minus some intercept usually).
But a problem is that these attributions don't tell us at all how exactly the prediction changes when changing the feature.


-->

## Strengths

**Ceteris paribus plots are super simple to implement and understand.**
This makes them a great entry point for beginners, but also for communicating model-agnostic explainability to others, especially non-experts.

**CP plots can fix limitations of attribution methods.**
Attribution-based methods like SHAP or LIME don't show how sensitive the prediction function is to local changes.
Ceteris paribus plots can complement attribution-based techniques and provide a complete picture when it comes to explaining individual predictions.

**Ceteris paribus plots are flexible building blocks.**
They are building blocks for other interpretation methods, but you can also get creative in combining these lines across models, classes, hyperparameter settings, and features to create nuanced insights into the model predictions.

## Limitations

**Ceteris paribus plots only show us one feature change at a time.**
This means we don't see how two features interact.
Of course, you can change two features, especially if one is continuous and the other binary, and plot them in the same CP plot.
But it's a more manual process.

**Interpretation suffers when features are correlated**.
When features are correlated, not all parts of the curve are likely or might even be completely unrealistic.
This can be alleviated by e.g. restricting the range of the ceteris paribus plots to shorter ranges, at least for correlated features.
But this would also mean we need a model or procedure to tell us what these ranges are.

In general, you must **be careful with causal interpretation**; or if you want one, make sure the model itself is causal.
This is a problem with all interpretation methods, but the risk of wrongful causal interpretation may be higher with CP plots since there is a lower barrier to showing the plots to non-experts.

## Software and alternatives

I created all plots in this chapter with the [ceterisParibus R package](https://cran.r-project.org/web/packages/ceterisParibus/index.html).
It also has a [Python implementation](https://github.com/ModelOriented/pyCeterisParibus).
You can further create CP plots with any tool that can produce ICE plots, like ICEBox and iml, by simply providing a "dataset" that only contains the one data point you are interested in.
However, the ceterisParibus package is better suited because it makes it simpler to compare ceteris paribus curves.


[^hate-latin]: I'm not a fan of the Latin name, because in school they promised that Latin helps with learning other Latin-based languages, like Italian. But you know what even helps better when your goal is to learn Italian? That's right. Just learn Italian.

[^curve-vs-plot]: By "CP curve" I mean a single line, and by "CP plot" I mean a graph showing one or more CP curves.

